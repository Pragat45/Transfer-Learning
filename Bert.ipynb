{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86472e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Core packages for text processing.\n",
    "\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ae71684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from wordcloud) (1.24.3)\n",
      "Requirement already satisfied: pillow in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from wordcloud) (9.4.0)\n",
      "Requirement already satisfied: matplotlib in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from wordcloud) (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (23.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "064615c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy) (2.6.4)\n",
      "Requirement already satisfied: jinja2 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy) (69.0.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from jinja2->spacy) (2.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96913732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pragatpagariya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/pragatpagariya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/pragatpagariya/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/pragatpagariya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from en_core_web_sm==2.2.5) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.6.4)\n",
      "Requirement already satisfied: jinja2 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (69.0.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.2.2->en_core_web_sm==2.2.5) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.2.2->en_core_web_sm==2.2.5) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from jinja2->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pragatpagariya/anaconda3/lib/python3.11/site-packages/spacy/util.py:918: UserWarning: [W094] Model 'en_core_web_sm' (2.2.5) specifies an under-constrained spaCy version requirement: >=2.2.2. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the \"spacy_version\" in your meta.json to a version range, with a lower and upper pin. For example: >=3.7.4,<3.8.0\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Loading some sklearn packaces for modelling.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# Some packages for word clouds and NER.\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from collections import Counter, defaultdict\n",
    "from PIL import Image\n",
    "import spacy\n",
    "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c634d468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from en_core_web_sm==2.2.5) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.6.4)\n",
      "Requirement already satisfied: jinja2 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (69.0.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.2.2->en_core_web_sm==2.2.5) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.2.2->en_core_web_sm==2.2.5) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/pragatpagariya/anaconda3/lib/python3.11/site-packages (from jinja2->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.tokenize import word_tokenize \n",
    "from collections import Counter, defaultdict\n",
    "from PIL import Image\n",
    "import spacy\n",
    "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz\n",
    "import en_core_web_sm\n",
    "\n",
    "# Core packages for general use throughout the notebook.\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2c1d5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Loading pytorch packages.\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57fa8ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "160abe0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>time</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599994</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599999 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label        time                          date     query  \\\n",
       "0            0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "1            0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "2            0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "3            0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4            0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "...        ...         ...                           ...       ...   \n",
       "1599994      4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599995      4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996      4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997      4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998      4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                username                                               text  \n",
       "0          scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "1               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "2                ElleCTF    my whole body feels itchy and like its on fire   \n",
       "3                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "4               joy_wolf                      @Kwesidei not the whole crew   \n",
       "...                  ...                                                ...  \n",
       "1599994  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599995      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599996           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599997     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599998   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1599999 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"twitter.csv\", encoding = \"ISO-8859-1\")\n",
    "data.columns = [\"label\", \"time\", \"date\", \"query\", \"username\", \"text\"]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "facd1da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        '['\n",
    "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+',\n",
    "        flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    return re.sub(html, '', text)\n",
    "\n",
    "\n",
    "def remove_punct(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "# Applying helper functions\n",
    "\n",
    "data['text_clean'] = data['text'].apply(lambda x: remove_URL(x))\n",
    "data['text_clean'] = data['text_clean'].apply(lambda x: remove_emoji(x))\n",
    "data['text_clean'] = data['text_clean'].apply(lambda x: remove_html(x))\n",
    "data['text_clean'] = data['text_clean'].apply(lambda x: remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5d9c0a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>time</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he cant update his Facebook by t...</td>\n",
       "      <td>[is, upset, that, he, cant, update, his, Faceb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>Kenichan I dived many times for the ball Manag...</td>\n",
       "      <td>[Kenichan, I, dived, many, times, for, the, ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[my, whole, body, feels, itchy, and, like, its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>nationwideclass no its not behaving at all im ...</td>\n",
       "      <td>[nationwideclass, no, its, not, behaving, at, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>Kwesidei not the whole crew</td>\n",
       "      <td>[Kwesidei, not, the, whole, crew]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label        time                          date     query       username  \\\n",
       "0      0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY  scotthamilton   \n",
       "1      0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY       mattycus   \n",
       "2      0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY        ElleCTF   \n",
       "3      0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         Karoli   \n",
       "4      0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY       joy_wolf   \n",
       "\n",
       "                                                text  \\\n",
       "0  is upset that he can't update his Facebook by ...   \n",
       "1  @Kenichan I dived many times for the ball. Man...   \n",
       "2    my whole body feels itchy and like its on fire    \n",
       "3  @nationwideclass no, it's not behaving at all....   \n",
       "4                      @Kwesidei not the whole crew    \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  is upset that he cant update his Facebook by t...   \n",
       "1  Kenichan I dived many times for the ball Manag...   \n",
       "2    my whole body feels itchy and like its on fire    \n",
       "3  nationwideclass no its not behaving at all im ...   \n",
       "4                       Kwesidei not the whole crew    \n",
       "\n",
       "                                           tokenized  \n",
       "0  [is, upset, that, he, cant, update, his, Faceb...  \n",
       "1  [Kenichan, I, dived, many, times, for, the, ba...  \n",
       "2  [my, whole, body, feels, itchy, and, like, its...  \n",
       "3  [nationwideclass, no, its, not, behaving, at, ...  \n",
       "4                  [Kwesidei, not, the, whole, crew]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokenized'] = data['text_clean'].apply(word_tokenize)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7369ef19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>time</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he cant update his Facebook by t...</td>\n",
       "      <td>[is, upset, that, he, cant, update, his, Faceb...</td>\n",
       "      <td>[is, upset, that, he, cant, update, his, faceb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>Kenichan I dived many times for the ball Manag...</td>\n",
       "      <td>[Kenichan, I, dived, many, times, for, the, ba...</td>\n",
       "      <td>[kenichan, i, dived, many, times, for, the, ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[my, whole, body, feels, itchy, and, like, its...</td>\n",
       "      <td>[my, whole, body, feels, itchy, and, like, its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>nationwideclass no its not behaving at all im ...</td>\n",
       "      <td>[nationwideclass, no, its, not, behaving, at, ...</td>\n",
       "      <td>[nationwideclass, no, its, not, behaving, at, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "      <td>Kwesidei not the whole crew</td>\n",
       "      <td>[Kwesidei, not, the, whole, crew]</td>\n",
       "      <td>[kwesidei, not, the, whole, crew]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label        time                          date     query       username  \\\n",
       "0      0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY  scotthamilton   \n",
       "1      0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY       mattycus   \n",
       "2      0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY        ElleCTF   \n",
       "3      0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         Karoli   \n",
       "4      0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY       joy_wolf   \n",
       "\n",
       "                                                text  \\\n",
       "0  is upset that he can't update his Facebook by ...   \n",
       "1  @Kenichan I dived many times for the ball. Man...   \n",
       "2    my whole body feels itchy and like its on fire    \n",
       "3  @nationwideclass no, it's not behaving at all....   \n",
       "4                      @Kwesidei not the whole crew    \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  is upset that he cant update his Facebook by t...   \n",
       "1  Kenichan I dived many times for the ball Manag...   \n",
       "2    my whole body feels itchy and like its on fire    \n",
       "3  nationwideclass no its not behaving at all im ...   \n",
       "4                       Kwesidei not the whole crew    \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [is, upset, that, he, cant, update, his, Faceb...   \n",
       "1  [Kenichan, I, dived, many, times, for, the, ba...   \n",
       "2  [my, whole, body, feels, itchy, and, like, its...   \n",
       "3  [nationwideclass, no, its, not, behaving, at, ...   \n",
       "4                  [Kwesidei, not, the, whole, crew]   \n",
       "\n",
       "                                               lower  \n",
       "0  [is, upset, that, he, cant, update, his, faceb...  \n",
       "1  [kenichan, i, dived, many, times, for, the, ba...  \n",
       "2  [my, whole, body, feels, itchy, and, like, its...  \n",
       "3  [nationwideclass, no, its, not, behaving, at, ...  \n",
       "4                  [kwesidei, not, the, whole, crew]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['lower'] = data['tokenized'].apply(\n",
    "    lambda x: [word.lower() for word in x])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7815ec0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.  \n",
    "    \n",
    "    device = torch.device('cuda')    \n",
    "\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19234b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data['label'].values\n",
    "idx = len(labels)\n",
    "combined = pd.concat([data])\n",
    "combined = combined.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea76fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82a57298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\n",
      "Tokenized:  ['is', 'upset', 'that', 'he', 'can', \"'\", 't', 'update', 'his', 'facebook', 'by', 'text', '##ing', 'it', '.', '.', '.', 'and', 'might', 'cry', 'as', 'a', 'result', 'school', 'today', 'also', '.', 'blah', '!']\n",
      "Token IDs:  [2003, 6314, 2008, 2002, 2064, 1005, 1056, 10651, 2010, 9130, 2011, 3793, 2075, 2009, 1012, 1012, 1012, 1998, 2453, 5390, 2004, 1037, 2765, 2082, 2651, 2036, 1012, 27984, 999]\n"
     ]
    }
   ],
   "source": [
    "print(' Original: ', combined[0])\n",
    "\n",
    "# Print the tweet split into tokens.\n",
    "\n",
    "print('Tokenized: ', tokenizer.tokenize(combined[0]))\n",
    "\n",
    "# Print the sentence mapped to token ID's.\n",
    "\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(combined[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0a127c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  230\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "\n",
    "for text in combined:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    \n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    \n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2807775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "token_lens = []\n",
    "\n",
    "for text in combined:\n",
    "    tokens = tokenizer.encode(text, max_length = 512)\n",
    "    token_lens.append(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce1e7246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(figsize=(14, 6))\n",
    "sns.distplot(token_lens, color='#e74c3c')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b70ee0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599999,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train= combined[:idx]\n",
    "test = combined[idx:]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a1d661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_map(sentence,labs='None'):\n",
    "    \n",
    "    \n",
    "    global labels\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    \n",
    "    for text in sentence:\n",
    "        #   \"encode_plus\" will:\n",
    "        \n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            text,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            truncation='longest_first', # Activate and control truncation\n",
    "                            max_length = 240,           # Max length according to our text data.\n",
    "                            pad_to_max_length = True, # Pad & truncate all sentences.\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the id list. \n",
    "        \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        \n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    \n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    if labs != 'None': # Setting this for using this definition for both train and test data so labels won't be a problem in our outputs.\n",
    "        labels = torch.tensor(labels)\n",
    "        return input_ids, attention_masks, labels\n",
    "    else:\n",
    "        return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe1cffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_masks, labels = tokenize_map(train, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7843c89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,279,999 training samples\n",
      "320,000 validation samples\n"
     ]
    }
   ],
   "source": [
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 80-20 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2b4fb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99a15b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data = TensorDataset(input_ids,attention_masks)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b56705a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-large-uncased', # Use the 124-layer, 1024-hidden, 16-heads, 340M parameters BERT model with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification. You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the device which we set GPU in our case.\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a198fbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 393 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 1024)\n",
      "bert.embeddings.position_embeddings.weight               (512, 1024)\n",
      "bert.embeddings.token_type_embeddings.weight               (2, 1024)\n",
      "bert.embeddings.LayerNorm.weight                             (1024,)\n",
      "bert.embeddings.LayerNorm.bias                               (1024,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight        (1024, 1024)\n",
      "bert.encoder.layer.0.attention.self.query.bias               (1024,)\n",
      "bert.encoder.layer.0.attention.self.key.weight          (1024, 1024)\n",
      "bert.encoder.layer.0.attention.self.key.bias                 (1024,)\n",
      "bert.encoder.layer.0.attention.self.value.weight        (1024, 1024)\n",
      "bert.encoder.layer.0.attention.self.value.bias               (1024,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight      (1024, 1024)\n",
      "bert.encoder.layer.0.attention.output.dense.bias             (1024,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight       (1024,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias         (1024,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight          (4096, 1024)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (4096,)\n",
      "bert.encoder.layer.0.output.dense.weight                (1024, 4096)\n",
      "bert.encoder.layer.0.output.dense.bias                       (1024,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                 (1024,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                   (1024,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                (1024, 1024)\n",
      "bert.pooler.dense.bias                                       (1024,)\n",
      "classifier.weight                                          (2, 1024)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print('{:<55} {:>12}'.format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1332ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    optimizer = AdamW(model.parameters(),\n",
    "                  lr = 6e-6, \n",
    "                  eps = 1e-8 \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6933e7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = 3\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fc318ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    \n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    return accuracy_score(labels_flat, pred_flat)\n",
    "\n",
    "def flat_f1(preds, labels):\n",
    "    \n",
    "    \n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    return f1_score(labels_flat, pred_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4743838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):    \n",
    "    elapsed_rounded = int(round((elapsed))) \n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3ce052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n"
     ]
    }
   ],
   "source": [
    "# We'll store a number of quantities such as training and validation loss, validation accuracy, f1 score and timings.\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    print('')\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    \n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    \n",
    "    total_train_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "        \n",
    "        b_input_ids = batch[0].to(device).to(torch.int64)\n",
    "        b_input_mask = batch[1].to(device).to(torch.int64)\n",
    "        b_labels = batch[2].to(device).to(torch.int64)\n",
    "        \n",
    "        model.zero_grad()        \n",
    "        \n",
    "        loss, logits = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print('')\n",
    "    print('  Average training loss: {0:.2f}'.format(avg_train_loss))\n",
    "    print('  Training epcoh took: {:}'.format(training_time))\n",
    "    print('')\n",
    "    print('Running Validation...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    total_eval_f1 = 0\n",
    "    nb_eval_steps = 0\n",
    "    \n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "\n",
    "            \n",
    "            (loss, logits) = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "        \n",
    "        total_eval_loss += loss.item()\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        \n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        total_eval_f1 += flat_f1(logits, label_ids)\n",
    "    \n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print('  Accuracy: {0:.2f}'.format(avg_val_accuracy))\n",
    "    \n",
    "    # Report the final f1 score for this validation run.\n",
    "    \n",
    "    avg_val_f1 = total_eval_f1 / len(validation_dataloader)\n",
    "    print('  F1: {0:.2f}'.format(avg_val_f1))\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print('  Validation Loss: {0:.2f}'.format(avg_val_loss))\n",
    "    print('  Validation took: {:}'.format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    \n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Val_F1' : avg_val_f1,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print('')\n",
    "print('Training complete!')\n",
    "\n",
    "print('Total training took {:} (h:mm:ss)'.format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7660cc05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
